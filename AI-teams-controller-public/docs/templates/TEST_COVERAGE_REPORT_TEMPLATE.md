# Test Coverage Report

**Sprint**: [Sprint Number]
**Report Date**: [YYYY-MM-DD]
**Reporting Period**: [YYYY-MM-DD] to [YYYY-MM-DD]
**Generated By**: [DK/Automated CI]

---

## Executive Summary

**Overall Coverage**: [X.X%]
- **Backend**: [Y.Y%]
- **Frontend**: [Z.Z%]

**Coverage Trend**: [⬆️ +X% | ⬇️ -X% | → Stable] from last sprint

**Status**: ✅ [Target Met] | ⚠️ [Approaching Target] | ❌ [Below Target]

**New Test Metrics**:
- Tests Added: [N] (+[M%] from last sprint)
- Tests Updated: [N]
- Tests Removed: [N]

---

## Backend Test Coverage

### Overall Metrics

```
Coverage Summary:
├─ Lines:      X% (X/X lines)
├─ Branches:   Y% (Y/Y branches)
├─ Functions:  Z% (Z/Z functions)
└─ Statements: W% (W/W statements)
```

**File-Level Breakdown**:

| Module | File | Lines | Branches | Functions | Status |
|--------|------|-------|----------|-----------|--------|
| `app/api/` | `routes.py` | Y% | Y% | Y% | ✅/⚠️/❌ |
| `app/services/` | `template_service.py` | Y% | Y% | Y% | ✅/⚠️/❌ |
| `app/schemas/` | `team.py` | Y% | Y% | Y% | ✅/⚠️/❌ |

### Backend Test Files

**Unit Tests**: `backend/tests/`

| Test File | Test Count | Passing | Failing | Status |
|-----------|-----------|---------|---------|--------|
| `test_template_service.py` | 13 | 13 | 0 | ✅ PASS |
| `test_voice_routes.py` | 8 | 8 | 0 | ✅ PASS |
| `test_schemas.py` | 5 | 5 | 0 | ✅ PASS |

**Total Backend Tests**: [N] | **Pass Rate**: [X%]

### Backend Test Quality

**Test Categories**:
- **Unit Tests**: [N] tests ([X%])
- **Integration Tests**: [N] tests ([Y%])
- **Mocking Strategy**: [Description of mock approach]

**Coverage Gaps** (below 80%):
- `app/services/celery_tasks.py`: [X%] - [Reason: Expensive LLM calls, using syntax-only tests]
- `app/services/voice_routes.py`: [Y%] - [Reason for gap]

---

## Frontend Test Coverage

### Overall Metrics

```
Coverage Summary:
├─ Statements: X% (X/X statements)
├─ Branches:   Y% (Y/Y branches)
├─ Functions:  Z% (Z/Z functions)
└─ Lines:      W% (W/W lines)
```

**File-Level Breakdown**:

| Component Path | File | Statements | Branches | Functions | Status |
|----------------|------|-----------|----------|-----------|--------|
| `components/controller/` | `TmuxController.tsx` | Y% | Y% | Y% | ✅/⚠️/❌ |
| `components/` | `CreateTeamDialog.tsx` | Y% | Y% | Y% | ✅/⚠️/❌ |
| `hooks/` | `useVoiceRecorder.ts` | Y% | Y% | Y% | ✅/⚠️/❌ |

### Frontend Test Files

**Unit Tests**: `frontend/__tests__/`

| Test File | Test Count | Passing | Failing | Status |
|-----------|-----------|---------|---------|--------|
| `components/__tests__/TmuxController.test.tsx` | 12 | 12 | 0 | ✅ PASS |
| `hooks/__tests__/useVoiceRecorder.test.ts` | 8 | 8 | 0 | ✅ PASS |
| `lib/__tests__/api.test.ts` | 6 | 6 | 0 | ✅ PASS |

**Total Frontend Tests**: [N] | **Pass Rate**: [X%]

### Frontend Test Quality

**Test Categories**:
- **Unit Tests**: [N] tests ([X%])
- **Component Tests**: [N] tests ([Y%])
- **Hook Tests**: [N] tests ([Z%])
- **Integration Tests**: [N] tests ([W%])

**Mocking Strategy**:
- API mocks: [jest.mock approach]
- WebSocket mocks: [How WebSocket is mocked]
- Context providers: [How providers are mocked]

**Coverage Gaps** (below 80%):
- `hooks/useVoiceRecorder.ts`: [X%] - [Reason: WebSocket mock limitations]
- `components/TmuxController.tsx`: [Y%] - [Reason for gap]

---

## Untested/Hard-to-Test Code

**Accepted Non-Coverage Areas** (intentional):

| Component | Reason | Coverage | Risk Level |
|-----------|--------|----------|-----------|
| `celery_tasks.py` | Expensive LLM API calls | [0%] | Low (syntax-only) |
| `WebSocket handlers` | Integration-heavy | [X%] | Low (manual testing) |
| `UI animations` | Visual regression | [Y%] | Low (visual testing) |

**Justification**:
- Expensive operations (LLM API, TTS) use syntax validation only
- Real API integration tested manually on staging
- Visual regression tested via Playwright/UI automation

---

## Test Execution Metrics

### Performance

```
Backend Tests:
├─ Total Time: X min Y sec
├─ Slowest Test: test_... (Z.Z seconds)
└─ Average Time: Y ms/test

Frontend Tests:
├─ Total Time: A min B sec
├─ Slowest Test: test_... (C.C seconds)
└─ Average Time: D ms/test
```

### CI/CD Integration

**Automated Test Runs**:
- **Trigger**: [On PR / On commit / Scheduled]
- **Last Run**: [YYYY-MM-DD HH:MM]
- **Status**: ✅ All Passed | ⚠️ Some Failed | ❌ Critical Failure
- **Coverage Check**: ✅ Passed ([Target: X%]) | ❌ Failed (below X%)

---

## Test Quality Assessment

### Mutation Testing (if run)

**Mutation Score**: [X%]
- **Killed Mutations**: [N] (good tests caught the change)
- **Survived Mutations**: [N] (tests missed the change)
- **Timeout/Errors**: [N]

**Interpretation**:
- >80%: Excellent test quality
- 60-80%: Good, but some edge cases may be missed
- <60%: Weak tests, gaps likely exist

### Code Review Observations

**Test Quality Issues Found**:
- [Issue and recommendation]
- [Issue and recommendation]

**Testing Best Practices Followed**:
- ✅ Arrange-Act-Assert pattern used consistently
- ✅ Test names are descriptive
- ✅ Mocks are properly isolated
- ✅ No test interdependencies

---

## Trend Analysis

### Coverage Trend (Last 4 Sprints)

| Sprint | Backend | Frontend | Avg | Trajectory |
|--------|---------|----------|-----|-----------|
| Sprint N-3 | X% | Y% | (X+Y)/2 | — |
| Sprint N-2 | X% | Y% | (X+Y)/2 | ⬆️/⬇️/→ |
| Sprint N-1 | X% | Y% | (X+Y)/2 | ⬆️/⬇️/→ |
| Sprint N | X% | Y% | (X+Y)/2 | ⬆️/⬇️/→ |

**Analysis**:
- [Coverage improving/declining because...]
- [Areas consistently lagging]

---

## Compliance & Standards

**Coverage Target**: [X%]
- **Backend Target**: [Y%]
- **Frontend Target**: [Z%]

**Status**:
- ✅ Backend: [Above/At/Below] target
- ✅ Frontend: [Above/At/Below] target

**Standards Compliance**:
- ✅ TDD workflow enforced (tests written first)
- ✅ Code review requires test sign-off
- ✅ New code requires 80%+ coverage

---

## Action Items

**For Next Sprint**:

| Priority | Action | Owner | Target Coverage | Due |
|----------|--------|-------|-----------------|-----|
| HIGH | Improve `service_X.py` coverage from X% to 85% | BE | 85% | Sprint N+1 |
| HIGH | Add missing component tests for `ComponentY` | FE | 80% | Sprint N+1 |
| MEDIUM | Refactor `untestable_function` to be more testable | BE | 75% → 90% | Sprint N+1 |

**Test Infrastructure Improvements**:
- [ ] [Improvement]: [Description] (Owner: [Role])
- [ ] [Improvement]: [Description] (Owner: [Role])

---

## Appendix: Test Execution Details

### Failed Tests (if any)

```
FAILED backend/tests/test_service.py::test_scenario_x
  AssertionError: Expected X but got Y
  Details: ...

FAILED frontend/__tests__/component.test.tsx::test_scenario_y
  Error: [Error message]
  Details: ...
```

**Root Cause Analysis**:
- [Test 1]: [Root cause], [Fix applied/planned]
- [Test 2]: [Root cause], [Fix applied/planned]

### Flaky Tests Identified

| Test | Flakiness | Investigation | Status |
|------|-----------|----------------|--------|
| `test_X` | [Fails 1 in N runs] | [Investigation notes] | ✅ Fixed / ⏳ In Progress |

---

## Glossary

- **Coverage**: % of code executed by tests
- **Branch Coverage**: % of conditional branches tested
- **Statement Coverage**: % of individual statements executed
- **Mutation Testing**: Simulates code changes to verify test quality
- **Flaky Test**: Test that passes/fails inconsistently
